# -*- coding: utf-8 -*-
"""3.NLP with spaCy and Textacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gqiH1J54_wDiy7VpR-_zRPkMUdHbGgmF

#Leveraging Linguistics
Getting Started
"""

#!conda install -y spacy 
#!pip install spacy

!python -m spacy download en_core_web_lg

import spacy
nlp = spacy.load('en_core_web_lg')

spacy.__version__

import spacy.cli
spacy.cli.download("en_core_web_lg")
import en_core_web_lg
nlp = en_core_web_lg.load()

#!conda install -c conda-forge textacy 
#!pip install textacy

import textacy

"""#Redacting Names with Named Entity Recognition"""

text = "Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy."

# Parse the text with spaCy. This runs the entire NLP pipeline.
doc = nlp(text)

for entity in doc.ents:
    print(f"{entity.text} ({entity.label_})")

doc.ents

entity.label, entity.label_

spacy.explain('GPE')

def redact_names(text):
    doc = nlp(text)
    redacted_sentence = []
    for token in doc:
        if token.ent_type_ == "PERSON":
            redacted_sentence.append("[REDACTED]")
        else:
            redacted_sentence.append(token.string)
    return "".join(redacted_sentence)

print(redact_names(text))

def redact_names(text):
    doc = nlp(text)
    redacted_sentence = []
    for ent in doc.ents:
        ent.merge()
    for token in doc:
        if token.ent_type_ == "PERSON":
            redacted_sentence.append("[REDACTED]")
        else:
            redacted_sentence.append(token.string)
    return "".join(redacted_sentence)

redact_names(text)

"""#Entity Types

Let's look at some examples of above in real world sentences, we will also use the spacy.explain() on all entities to build a quick mental model of how these things work.
"""

def explain_text_entities(text):
    doc = nlp(text)
    for ent in doc.ents:
        print(f'{ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')



explain_text_entities('Buzz Aldrin (born January 20, 1930) is an American former astronaut and fighter pilot. As lunar module pilot on the Apollo 11 mission, he and Neil Armstrong were the first humans to land on the Moon. A graduate of West Point and MIT, where he earned a doctorate in astronautics, Aldrin served as an Air Force fighter pilot during the Korean War, flying 66 combat missions and shooting down two MiGs. ')

explain_text_entities('Tesla has gained 20% market share in the months since')

explain_text_entities('Ashoka was a great Indian king')

explain_text_entities('The Ashoka University sponsors the Young India Fellowship')

"""#Automatic Question Generation

##Part-of-Speech Tagging
"""

example_text = 'Bansoori is an Indian classical instrument. Tom plays Bansoori and Guitar.'

doc = nlp(example_text)

"""We need noun chunks. Noun chunks are noun phrases - not a single word, but a short phrase which describes the noun. For example, "the blue skies" or "the worldâ€™s largest conglomerate".

To get the noun chunks in a document, simply iterate over doc.noun_chunks:
"""

for idx, sentence in enumerate(doc.sents):
    for noun in sentence.noun_chunks:
        print(f'sentence{idx+1}', noun)

for token in doc:
    print(token, token.pos_, token.tag_)

"""**Creating a Ruleset**

Quite often when using linguistics, you will be writing custom rules. Here is one data structure suggestion to help you store these rules: list of dictionaries. Each dictionary in turn can have elements ranging from simple string lists to lists to strings. Avoid nesting a list of dictionaries inside a dictionary:
"""

ruleset = [
    {
        'id': 1, 
        'req_tags': ['NNP', 'VBZ', 'NN'],
    }, 
    {
        'id': 2, 
        'req_tags': ['NNP', 'VBZ'],
    }
    ]

print(ruleset)

def get_pos_tag(doc, tag):
    return [tok for tok in doc if tok.tag_ == tag]

def sent_to_ques(sent:str)->str:
    """
    Return a question string corresponding to a sentence string using a set of pre-written rules
    """
    doc = nlp(sent)
    pos_tags = [token.tag_ for token in doc]
    for idx, rule in enumerate(ruleset):
        if rule['id'] == 1:
            if all(key in pos_tags for key in rule['req_tags']): 
                print(f"Rule id {rule['id']} matched for sentence: {sent}")
                NNP = get_pos_tag(doc, "NNP")
                NNP = str(NNP[0])
                VBZ = get_pos_tag(doc, "VBZ")
                VBZ = str(VBZ[0])
                ques = f'What {VBZ} {NNP}?'
                return(ques)
        if rule['id'] == 2:
            if all(key in pos_tags for key in rule['req_tags']): #'NNP', 'VBZ' in sentence.
                print(f"Rule id {rule['id']} matched for sentence: {sent}")
                NNP = get_pos_tag(doc, "NNP")
                NNP = str(NNP[0])
                VBZ = get_pos_tag(doc, "VBZ")
                VBZ = str(VBZ[0].lemma_)
                ques = f'What does {NNP} {VBZ}?'
                return(ques)

for sent in doc.sents:
    print(f"The generated question is: {sent_to_ques(str(sent))}")

"""# Question Generation using Dependency Parsing"""

for token in doc:
    print(token, token.dep_)

for token in doc:
    print(token, token.dep_, spacy.explain(token.dep_))

"""#Visualizing the Relationship

spaCy has an inbuilt tool called displacy for displaying simple, but clean and powerful visualizations. It offers two primary modes: Named Entity Recognition and Dependency Parsing. Here we will use the 'dep' or dependency mode.
"""

displacy.render(doc, style='dep', jupyter=True)

tricky_doc = nlp('This is ship-shipping ship, shipping shipping ships')

displacy.render(tricky_doc, style='dep', jupyter=True)

from textacy.spacier import utils as spacy_utils

??spacy_utils.get_main_verbs_of_sent

# Signature: spacy_utils.get_main_verbs_of_sent(sent)
# Source:   
# def get_main_verbs_of_sent(sent):
#     """Return the main (non-auxiliary) verbs in a sentence."""
#     return [tok for tok in sent
#             if tok.pos == VERB and tok.dep_ not in constants.AUX_DEPS]
# File:      d:\miniconda3\envs\nlp\lib\site-packages\textacy\spacier\utils.py
# Type:      function

toy_sentence = 'Shivangi is an engineer'
doc = nlp(toy_sentence)

"""What are the entities in this sentence?"""

displacy.render(doc, style='ent', jupyter=True)

# Let's find out the main verb in this sentence:

verbs = spacy_utils.get_main_verbs_of_sent(doc)
print(verbs)

# And what are nominal subjects of this verb?

 
for verb in verbs:
    print(verb, spacy_utils.get_subjects_of_verb(verb))

"""*You will notice that this has a reasonable overlap with the noun phrases which we pulled from our part-of-speech tagging but can be different as well.*"""

[(token, token.tag_) for token in doc]

"""Tip: As an exercise, extend this approach to at least add Who, Where and When questions as practice.

## Level Up: Question and Answer

So far, we have been trying to generate questions. But if you were trying to make an automated quiz for students, you would also need to mine the right answer.

The answer in this case will be simply the objects of verb. What is an object of verb?

In the sentence, "Give the book to me," "book" is the direct object of the verb "give," and "me" is the indirect object. - from the Cambridge English Dictionary

Loosely, object is the piece on which our verb acts. This is almost always the answer to our "what". Let's write a question to find the objects of any verb --- or wait, we can pull it from the textacy.spacier.utils.
"""

spacy_utils.get_objects_of_verb(verb)

for verb in verbs:
    print(verb, spacy_utils.get_objects_of_verb(verb))

displacy.render(doc, style='dep', jupyter=True)

"""Let's look at the output of our functions for the example text. The first is the sentence itself, then the root verb, than the lemma form of that verb, followed by subjects of the verb and then objects."""

doc = nlp(example_text)
for sentence in doc.sents:
    print(sentence, sentence.root, sentence.root.lemma_, spacy_utils.get_subjects_of_verb(sentence.root), spacy_utils.get_objects_of_verb(sentence.root))

"""Bansoori is an Indian classical instrument. is be [Bansoori] [instrument]
Tom plays Bansoori and Guitar. plays play [Tom] [Bansoori, Guitar]
Let's arrange the pieces above into a neat function which we can then re-use
"""

def para_to_ques(eg_text):
    doc = nlp(eg_text)
    results = []
    for sentence in doc.sents:
        root = sentence.root
        ask_about = spacy_utils.get_subjects_of_verb(root)
        answers = spacy_utils.get_objects_of_verb(root)
        if len(ask_about) > 0 and len(answers) > 0:
            if root.lemma_ == "be":
                question = f'What {root} {ask_about[0]}?'
            else:
                question = f'What does {ask_about[0]} {root.lemma_}?'
            results.append({'question':question, 'answers':answers})
    return results

para_to_ques(example_text)

"""This seems right to me. Let's run this on a larger sample of sentences. This sample has varying degrees of complexities and sentence structures."""

large_example_text = """
Puliyogare is a South Indian dish made of rice and tamarind. 
Priya writes poems. Shivangi bakes cakes. Sachin sings in the orchestra.

Osmosis is the movement of a solvent across a semipermeable membrane toward a higher concentration of solute. In biological systems, the solvent is typically water, but osmosis can occur in other liquids, supercritical liquids, and even gases.
When a cell is submerged in water, the water molecules pass through the cell membrane from an area of low solute concentration to high solute concentration. For example, if the cell is submerged in saltwater, water molecules move out of the cell. If a cell is submerged in freshwater, water molecules move into the cell.

Raja-Yoga is divided into eight steps. The first is Yama. Yama is nonviolence, truthfulness, continence, and non-receiving of any gifts.
After Yama, Raja-Yoga has Niyama. cleanliness, contentment, austerity, study, and self - surrender to God.
The steps are Yama and Niyama. 
"""

para_to_ques(large_example_text)

"""Facts Extraction using Semi Structured Sentence Parsing
Introducing textacy,

**Boss mode with co reference resolution**
"""





