# -*- coding: utf-8 -*-
"""4 Text Representations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LYLAbtTrtKojQ8znQxusjFA7EKeesDAT
"""

import gensim
print(f'gensim: {gensim.__version__}')

!conda install -y tqdm

"""Solving environment: ...working... done

# All requested packages already installed.
"""

from tqdm import tqdm
class TqdmUpTo(tqdm):
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None: self.total = tsize
        self.update(b * bsize - self.n)

def get_data(url, filename):
    """
    Download data if the filename does not exist already
    Uses Tqdm to show download progress
    """
    import os
    from urllib.request import urlretrieve
    
    if not os.path.exists(filename):

        dirname = os.path.dirname(filename)
        if not os.path.exists(dirname):
            os.makedirs(dirname)

        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:
            urlretrieve(url, filename, reporthook=t.update_to)

embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'
get_data(embedding_url, 'data/glove.6B.zip')

# # We need to run this only once, can unzip manually unzip to the data directory too
!unzip data/glove.6B.zip 
!mv glove.6B.300d.txt data/glove.6B.300d.txt 
!mv glove.6B.200d.txt data/glove.6B.200d.txt 
!mv glove.6B.100d.txt data/glove.6B.100d.txt 
!mv glove.6B.50d.txt data/glove.6B.50d.txt

from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'data/glove.6B.300d.txt'

word2vec_output_file = 'data/glove.6B.300d.word2vec.txt'

import os
if not os.path.exists(word2vec_output_file):
    glove2word2vec(glove_input_file, word2vec_output_file)

"""##KeyedVectors API"""

from gensim.models import KeyedVectors
filename = word2vec_output_file

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # load the Stanford GloVe model from file, this is Disk I/O and can be slow
# pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)
# # binary=False format for human readable text (.txt) files, and binary=True for .bin files

# calculate: (king - man) + woman = ?
result = pretrained_w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(result)

# calculate: (india - canada) +  = ?
result = pretrained_w2v_model.most_similar(positive=['quora', 'facebook'], negative=['linkedin'], topn=1)
print(result)

#pretrained_w2v_model.most_similar('bang;adesh')
#pretrained_w2v_model.most_similar('love')
pretrained_w2v_model.most_similar('india')

"""**What is missing in both word2vec and GloVe?**"""

try:
    pretrained_w2v_model.wv.most_similar('nirant')
except Exception as e:
    print(e)

"""**How to handle OOV words?**"""

ted_dataset = "https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip"
get_data(ted_dataset, "data/ted_en.zip")

import zipfile
import lxml.etree
with zipfile.ZipFile('data/ted_en.zip', 'r') as z:
    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))
input_text = '\n'.join(doc.xpath('//content/text()'))

#print(input_text[:5000])
print(input_text[:500])

import re
# remove parenthesis 
input_text_noparens = re.sub(r'\([^)]*\)', '', input_text)

# store as list of sentences
sentences_strings_ted = []
for line in input_text_noparens.split('\n'):
    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)
    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)

# store as list of lists of words
sentences_ted = []
for sent_str in sentences_strings_ted:
    tokens = re.sub(r"[^a-z0-9]+", " ", sent_str.lower()).split()
    sentences_ted.append(tokens)

#print(sentences_ted[:5])
sentences_ted[:2]

"""#create a json file"""

import json
with open('ted_clean_sentences.json', 'w') as fp:
    json.dump(sentences_ted, fp)

with open('ted_clean_sentences.json', 'r') as fp:
    sentences_ted = json.load(fp)

print(sentences_ted[:2])

"""#Train FastText Embedddings"""

from gensim.models.fasttext import FastText

# Commented out IPython magic to ensure Python compatibility.
# %%time
# fasttext_ted_model = FastText(sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)
# # sg = 1 denotes skipgram, else CBOW is used

fasttext_ted_model.wv.most_similar("india")

"""#Train word2vec Embeddings"""

from gensim.models.word2vec import Word2Vec

# Commented out IPython magic to ensure Python compatibility.
# %%time
# word2vec_ted_model = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)

word2vec_ted_model.wv.most_similar("india")

"""**fastText or word2vec?**

#Document Embeddings
"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import gensim
from pprint import pprint
import multiprocessing

import zipfile
import lxml.etree
with zipfile.ZipFile('data/ted_en.zip', 'r') as z:
    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))
    
talks = doc.xpath('//content/text()')

def read_corpus(talks, tokens_only=False):
    for i, line in enumerate(talks):
        if tokens_only:
            yield gensim.utils.simple_preprocess(line)
        else:
            # For training data, add tags
            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])

read_corpus(talks)

ted_talk_docs = list(read_corpus(talks))

print(ted_talk_docs[0])

cores = multiprocessing.cpu_count()
print(cores)

model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, epochs=5, workers=cores)

# Commented out IPython magic to ensure Python compatibility.
# %time model.build_vocab(ted_talk_docs)

sentence_1 = 'Modern medicine has changed the way we think about healthcare, life spans and by extension career and marriage'

sentence_2 = 'Modern medicine is not just a boon to the rich, making the raw chemicals behind these is also pollutes the poorest neighborhoods'

sentence_3 = 'Modern medicine has changed the way we think about healthcare, and increased life spans, delaying weddings'

model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())

model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())

# Commented out IPython magic to ensure Python compatibility.
# %time model.train(ted_talk_docs, total_examples=model.corpus_count, epochs=model.epochs)

model.infer_vector(sentence_1.split())

model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())

model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())

model.docvecs.similarity_unseen_docs(model, sentence_2.split(), sentence_3.split())

"""# Model Assessment"""

ranks = []
for idx in range(len(ted_talk_docs)):
    inferred_vector = model.infer_vector(ted_talk_docs[idx].words)
    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))
    rank = [docid for docid, sim in sims].index(idx)
    ranks.append(rank)

import collections
collections.Counter(ranks)  # Results vary due to random seeding + very small corpus

doc_slice = ' '.join(ted_talk_docs[idx].words)[:500]
print(f'Document ({idx}): «{doc_slice}»\n')
print(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}')
for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
      doc_slice = ' '.join(ted_talk_docs[sims[index][0]].words)[:500]
      print(f'{label} {sims[index]}: «{doc_slice}»\n')







