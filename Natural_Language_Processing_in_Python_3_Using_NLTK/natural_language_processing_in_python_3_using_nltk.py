# -*- coding: utf-8 -*-
"""Natural Language Processing in Python 3 Using NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X2H50Q-74bRjGHVUWI-sCnRK1X42tmP1
"""



!pip install nltk

# version check
 nltk.__version__

# Importing the Library
import nltk
nltk.download('twitter_samples')

from nltk.corpus import twitter_samples
twitter_samples.fileids()

text = twitter_samples.strings(
        'tweets.20150430-223406.json')
len(text)

"""#Pre-Processing Text

**Word Tokenization**
In its natural form, it is difficult to programmatically analyze textual data. You must, therefore, convert text into smaller parts called tokens.
"""

twitter_samples.tokenized('tweets.20150430-223406.json')[10]

nltk.download('punkt')

from nltk.tokenize import word_tokenize
word_tokenize(text[10])

"""##Converting Words to their Canonical Form"""

from nltk.stem.porter import PorterStemmer 
stem = PorterStemmer()
stem.stem('swimming')

stem.stem('swam')

nltk.download('wordnet')

# use the lemmatizer.
from nltk.stem.wordnet import WordNetLemmatizer 
lem = WordNetLemmatizer()
lem.lemmatize('swim', 'v')

nltk.download('averaged_perceptron_tagger')

from nltk.tag import pos_tag
sample = "Your time is limited, so don't waste it living someone else's life."
#sample = "i am GPA 5"
pos_tag(word_tokenize(sample))

# For every word, the tagger returns a string. How do you make sense of the tags?
nltk.download('tagsets')

nltk.help.upenn_tagset()

def lemmatize_sentence(sentence):
    lemmatizer = WordNetLemmatizer()
    lemmatized_sentence = []
    for word, tag in pos_tag(word_tokenize(sentence)):
        if tag.startswith('NN'):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'
        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))
    return lemmatized_sentence

#  now ready to use the lemmatizer.
lemmatize_sentence(sample)

"""# Remove Noise

**For the sample tweets,**

 you should remove the following â€” -

All hyperlinks, which would not add any value to the analysis

Twitter handles in replies

Punctuation and special characters

To search for each of the above items and remove them, you will use the regular expressions library in Python, through the package re.
"""

import re
sample = 'Go to https://alibabacloud.com/campaign/techshare/ for tech tutorials'
re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|'\
       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', sample)

sample = 'Go to @alibaba for techshare tutorials'
re.sub('(@[A-Za-z0-9_]+)','', sample)

# remove any punctuation marks using the string library.
import string
string.punctuation

sample = 'Hi!!! How are you?'
sample.translate(str.maketrans('', '', string.punctuation))

nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
print(len(stop_words))

stop_words[:3]

def remove_noise(tokens, stop_words = ()):
    '''Remove @ mentions, hyperlinks, punctuation, and stop words'''
    clean_tokens = []
    lemmatizer = WordNetLemmatizer()
    for token, tag in pos_tag(tokens):
        # Remove Hyperlinks
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|'\
                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)
        # Remove twitter handles
        token = re.sub("(@[A-Za-z0-9_]+)","", token)
        
        if tag.startswith("NN"):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'
        # Normalize sentence
        token = lemmatizer.lemmatize(token, pos)
        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:
            # Get lowercase
            clean_tokens.append(token)
    return clean_tokens

"""# Word Density"""

tokens_list = twitter_samples.tokenized('tweets.20150430-223406.json')
clean_tokens_list = [remove_noise(tokens, stop_words) for tokens in tokens_list]

all_words = []
for tokens in clean_tokens_list:
    for token in tokens:
        all_words.append(token)

freq_dist = nltk.FreqDist(all_words)
freq_dist.most_common(15)

import matplotlib.pyplot as plt
items = freq_dist.most_common(10)
labels, values = zip(*items)
width = 0.5
plt.bar(labels, values, width, align='center', )
plt.show()

from wordcloud import WordCloud
cloud = WordCloud(max_font_size=60).generate(' '.join(all_words))
plt.figure(figsize=(16,12))
plt.imshow(cloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Named Entity Recognition"""

nltk.download('maxent_ne_chunker')

nltk.download('words')

#To find named entities in your text, you need to create chunks of the data as follows.
from nltk import ne_chunk, pos_tag
chunked = ne_chunk(pos_tag(clean_tokens_list[15]))

chunked.draw()

from collections import defaultdict
named_entities = defaultdict(list)
for node in chunked:
    # Check if node is a Tree
    # If not a tree, ignore
    if type(node) is nltk.tree.Tree:
        # Get the type of entity
        label = node.label()
        entity = node[0][0]
        named_entities[label].append(entity)

named_entities

"""# TF-IDF"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
import numpy as np
cv = CountVectorizer(min_df=0.005, max_df=.5, ngram_range=(1,2))
sentences = [' '.join(tokens) for tokens in clean_tokens_list]
cv.fit(sentences)

len(cv.vocabulary_)

cv_counts = cv.transform(sentences)

100.0 * cv_counts.nnz / (cv_counts.shape[0] * cv_counts.shape[1])

transformed_weights = TfidfTransformer().fit_transform(cv_counts)
features = {}
for feature, weight in zip(cv.get_feature_names(),
                           np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()):
    features[feature] = weight
sorted_features = [(key, features[key]) 
                   for key in sorted(features, key=features.get, reverse=True)]

sorted_features[:10]