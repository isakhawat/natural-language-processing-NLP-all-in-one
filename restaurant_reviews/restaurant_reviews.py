# -*- coding: utf-8 -*-
"""Restaurant_Reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uur3oVLlfyjqaKL9SbTpQU5n_cxUitrX
"""



# Importing Libraries 
import numpy as np   
import pandas as pd

# Import dataset 
dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t')  
print(dataset)

"""**step2: Text Cleaning or Preprocessing**"""

# library to clean data 
import re 

# Natural Language Tool Kit 
import nltk 

nltk.download('stopwords') 

# to remove stopword 
from nltk.corpus import stopwords 

# for Stemming propose 
from nltk.stem.porter import PorterStemmer 

# Initialize empty array 
# to append clean text 
corpus = [] 

# 1000 (reviews) rows to clean 
for i in range(0, 1000): 
	
	# column : "Review", row ith 
	review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) 
	
	# convert all cases to lower cases 
	review = review.lower() 
	
	# split to array(default delimiter is " ") 
	review = review.split() 
	
	# creating PorterStemmer object to 
	# take main stem of each word 
	ps = PorterStemmer() 
	
	# loop for stemming each word 
	# in string array at ith row	 
	review = [ps.stem(word) for word in review 
				if not word in set(stopwords.words('english'))] 
				
	# rejoin all string array elements 
	# to create back into a string 
	review = ' '.join(review) 
	
	# append each string to create 
	# array of clean text 
	corpus.append(review)

"""**Step 3: Tokenization, involves splitting sentences and words from the body of the text.**

**Step 4: Making the bag of words via sparse matrix**
"""

# Creating the Bag of Words model 
from sklearn.feature_extraction.text import CountVectorizer 

# To extract max 1500 feature. 
# "max_features" is attribute to 
# experiment with to get better results 
cv = CountVectorizer(max_features = 1500) 

# X contains corpus (dependent variable) 
X = cv.fit_transform(corpus).toarray() 

# y contains answers if review 
# is positive or negative 
y = dataset.iloc[:, 1].values

"""**Step 5 : Splitting Corpus into Training and Test set.** 

For this, we need class train_test_split from sklearn.cross_validation. Split can be made 70/30 or 80/20 or 85/15 or 75/25, here I choose 75/25 via “test_size”.
X is the bag of words, y is 0 or 1 (positive or negative).
"""

!pip install sklearn.cross_validation

# Splitting the dataset into 
# the Training set and Test set 
# from sklearn.cross_validation import train_test_split { its old not work}
from sklearn.model_selection import train_test_split

# experiment with "test_size" 
# to get better results 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

"""**Step 6: Fitting a Predictive Model (here random forest)**

Since Random fored is ensemble model (made of many trees) from sklearn.ensemble, import RandomForestClassifier class
With 501 tree or “n_estimators” and criterion as ‘entropy’
Fit the model via .fit() method with attributes X_train and y_train
filter_none
"""

# Fitting Random Forest Classification 
# to the Training set 
from sklearn.ensemble import RandomForestClassifier 

# n_estimators can be said as number of 
# trees, experiment with n_estimators 
# to get better results 
model = RandomForestClassifier(n_estimators = 501, 
							criterion = 'entropy') 
							
model.fit(X_train, y_train)

"""**Step 7: Pridicting Final Results via using .predict() method with attribute X_test**"""

# Predicting the Test set results 
y_pred = model.predict(X_test) 

y_pred

"""**Step 8: To know the accuracy, confusion matrix is needed.**

Confusion Matrix is a 2X2 Matrix.

**TRUE POSITIVE**: measures the proportion of actual positives that are correctly identified.

**TRUE NEGATIVE :** measures the proportion of actual positives that are not correctly identified.

**FALSE POSITIVE :** measures the proportion of actual negatives that are correctly identified.

**FALSE NEGATIVE :** measures the proportion of actual negatives that are not correctly identified.
"""

# Making the Confusion Matrix 
from sklearn.metrics import confusion_matrix 

cm = confusion_matrix(y_test, y_pred) 

cm









