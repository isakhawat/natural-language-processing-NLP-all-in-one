{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restaurant_Reviews.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLO1kmhH6qFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJ3zIsu5099",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries \n",
        "import numpy as np   \n",
        "import pandas as pd  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFDRgrEU77fy",
        "colab_type": "code",
        "outputId": "9eba9e32-75d6-4ffa-e7e0-b80a6857a9b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Import dataset \n",
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')  \n",
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Review  Liked\n",
            "0                             Wow... Loved this place.      1\n",
            "1                                   Crust is not good.      0\n",
            "2            Not tasty and the texture was just nasty.      0\n",
            "3    Stopped by during the late May bank holiday of...      1\n",
            "4    The selection on the menu was great and so wer...      1\n",
            "..                                                 ...    ...\n",
            "995  I think food should have flavor and texture an...      0\n",
            "996                           Appetite instantly gone.      0\n",
            "997  Overall I was not impressed and would not go b...      0\n",
            "998  The whole experience was underwhelming, and I ...      0\n",
            "999  Then, as if I hadn't wasted enough of my life ...      0\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "466VsYxO6md-",
        "colab_type": "text"
      },
      "source": [
        "**step2: Text Cleaning or Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5AA63r06vdm",
        "colab_type": "code",
        "outputId": "bbe81216-0c3b-4f63-c8d9-887f59241f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# library to clean data \n",
        "import re \n",
        "\n",
        "# Natural Language Tool Kit \n",
        "import nltk \n",
        "\n",
        "nltk.download('stopwords') \n",
        "\n",
        "# to remove stopword \n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "# for Stemming propose \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "\n",
        "# Initialize empty array \n",
        "# to append clean text \n",
        "corpus = [] \n",
        "\n",
        "# 1000 (reviews) rows to clean \n",
        "for i in range(0, 1000): \n",
        "\t\n",
        "\t# column : \"Review\", row ith \n",
        "\treview = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) \n",
        "\t\n",
        "\t# convert all cases to lower cases \n",
        "\treview = review.lower() \n",
        "\t\n",
        "\t# split to array(default delimiter is \" \") \n",
        "\treview = review.split() \n",
        "\t\n",
        "\t# creating PorterStemmer object to \n",
        "\t# take main stem of each word \n",
        "\tps = PorterStemmer() \n",
        "\t\n",
        "\t# loop for stemming each word \n",
        "\t# in string array at ith row\t \n",
        "\treview = [ps.stem(word) for word in review \n",
        "\t\t\t\tif not word in set(stopwords.words('english'))] \n",
        "\t\t\t\t\n",
        "\t# rejoin all string array elements \n",
        "\t# to create back into a string \n",
        "\treview = ' '.join(review) \n",
        "\t\n",
        "\t# append each string to create \n",
        "\t# array of clean text \n",
        "\tcorpus.append(review) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFrr94Dh7cWY",
        "colab_type": "text"
      },
      "source": [
        "**Step 3: Tokenization, involves splitting sentences and words from the body of the text.**\n",
        "\n",
        "**Step 4: Making the bag of words via sparse matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT31Au226vvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Bag of Words model \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "\n",
        "# To extract max 1500 feature. \n",
        "# \"max_features\" is attribute to \n",
        "# experiment with to get better results \n",
        "cv = CountVectorizer(max_features = 1500) \n",
        "\n",
        "# X contains corpus (dependent variable) \n",
        "X = cv.fit_transform(corpus).toarray() \n",
        "\n",
        "# y contains answers if review \n",
        "# is positive or negative \n",
        "y = dataset.iloc[:, 1].values \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53_X9y0m8RgR",
        "colab_type": "text"
      },
      "source": [
        "**Step 5 : Splitting Corpus into Training and Test set.** \n",
        "\n",
        "For this, we need class train_test_split from sklearn.cross_validation. Split can be made 70/30 or 80/20 or 85/15 or 75/25, here I choose 75/25 via “test_size”.\n",
        "X is the bag of words, y is 0 or 1 (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt0aAC5v8fqV",
        "colab_type": "code",
        "outputId": "8445a37a-384f-4191-98fc-51f7f9039d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install sklearn.cross_validation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn.cross_validation (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for sklearn.cross_validation\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9tKtYW6v60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into \n",
        "# the Training set and Test set \n",
        "# from sklearn.cross_validation import train_test_split { its old not work}\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# experiment with \"test_size\" \n",
        "# to get better results \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr-eFBPo899O",
        "colab_type": "text"
      },
      "source": [
        "**Step 6: Fitting a Predictive Model (here random forest)**\n",
        "\n",
        "Since Random fored is ensemble model (made of many trees) from sklearn.ensemble, import RandomForestClassifier class\n",
        "With 501 tree or “n_estimators” and criterion as ‘entropy’\n",
        "Fit the model via .fit() method with attributes X_train and y_train\n",
        "filter_none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0yjlkG36v48",
        "colab_type": "code",
        "outputId": "c00a6a9d-764d-48e1-c7dd-68c62af4ef9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Fitting Random Forest Classification \n",
        "# to the Training set \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "\n",
        "# n_estimators can be said as number of \n",
        "# trees, experiment with n_estimators \n",
        "# to get better results \n",
        "model = RandomForestClassifier(n_estimators = 501, \n",
        "\t\t\t\t\t\t\tcriterion = 'entropy') \n",
        "\t\t\t\t\t\t\t\n",
        "model.fit(X_train, y_train) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=501,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMqSgulG9Q6E",
        "colab_type": "text"
      },
      "source": [
        "**Step 7: Pridicting Final Results via using .predict() method with attribute X_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72z4oac56v1S",
        "colab_type": "code",
        "outputId": "090e836c-e4cd-48a7-fd62-bde030f4c3db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Predicting the Test set results \n",
        "y_pred = model.predict(X_test) \n",
        "\n",
        "y_pred \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfH__YAOCB43",
        "colab_type": "text"
      },
      "source": [
        "**Step 8: To know the accuracy, confusion matrix is needed.**\n",
        "\n",
        "Confusion Matrix is a 2X2 Matrix.\n",
        "\n",
        "**TRUE POSITIVE**: measures the proportion of actual positives that are correctly identified.\n",
        "\n",
        "**TRUE NEGATIVE :** measures the proportion of actual positives that are not correctly identified.\n",
        "\n",
        "**FALSE POSITIVE :** measures the proportion of actual negatives that are correctly identified.\n",
        "\n",
        "**FALSE NEGATIVE :** measures the proportion of actual negatives that are not correctly identified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mztccLBK6vuO",
        "colab_type": "code",
        "outputId": "94459532-820c-4243-f77b-2382a4e41fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Making the Confusion Matrix \n",
        "from sklearn.metrics import confusion_matrix \n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred) \n",
        "\n",
        "cm \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[107,  15],\n",
              "       [ 50,  78]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woMhRCTJ6vss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFVM_RoA6vo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0yHresD6vkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FLfcufF6viA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN1ucRC56vap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}