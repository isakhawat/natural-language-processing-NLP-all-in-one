{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy 101: Everything you need to know.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCVjqrren4Gx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yn3bI8_n9TP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "NAME\tDESCRIPTION\n",
        "**Tokenization**\tSegmenting text into words, punctuations marks etc.\n",
        "\n",
        "**Part-of-speech** (POS) Tagging\tAssigning word types to tokens, like verb or noun.\n",
        "\n",
        "**Dependency Parsing**\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "\n",
        "**Lemmatization**\tAssigning the base forms of words. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
        "\n",
        "**Sentence Boundary Detection (SBD)**\tFinding and segmenting individual sentences.\n",
        "\n",
        "**Named Entity Recognition (NER)**\tLabelling named “real-world” objects, like persons, companies or locations.\n",
        "\n",
        "**Entity Linking (EL)**\tDisambiguating textual entities to unique identifiers in a Knowledge Base.\n",
        "\n",
        "**Similarity**\tComparing words, text spans and documents and how similar they are to each other.\n",
        "\n",
        "**Text Classification**\tAssigning categories or labels to a whole document, or parts of a document.\n",
        "\n",
        "**Rule-based Matching**\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
        "\n",
        "**Training**\tUpdating and improving a statistical model’s predictions.\n",
        "\n",
        "**Serialization**\tSaving objects to files or byte strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BISEua1quND",
        "colab_type": "text"
      },
      "source": [
        "**spacy.load():**  return a Language object "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2t1ouiOpzdE",
        "colab_type": "code",
        "outputId": "e66106e1-69ed-48cd-e4fa-4e761d8689f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "    #print(token.pos_)\n",
        "    #print(token.dep_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple PROPN nsubj\n",
            "is VERB aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.K. PROPN compound\n",
            "startup NOUN dobj\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "1 NUM compound\n",
            "billion NUM pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1QaSL30pzmA",
        "colab_type": "code",
        "outputId": "8a3967a1-1e53-485c-905a-6974b62ab374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(token.pos_)\n",
        "print(token.dep_)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NUM\n",
            "pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IwvISHUq9iw",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4j-AxrgrT_9",
        "colab_type": "code",
        "outputId": "a07ad8d7-95e8-469f-b67f-752352fc8618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6791a56f7bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Apple is looking at buying U.K. startup for $1 billion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtJCVy4Zq9Hp",
        "colab_type": "code",
        "outputId": "a4e67126-a2c8-43a8-bbe2-78082829171e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydzeG8eVsV5o",
        "colab_type": "text"
      },
      "source": [
        "# Part-of-speech tags and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvdkwYIhpzrm",
        "colab_type": "code",
        "outputId": "0108b95f-99c6-44a4-c95b-01acae7a9cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
            "is be VERB VBZ aux xx True True\n",
            "looking look VERB VBG ROOT xxxx True False\n",
            "at at ADP IN prep xx True True\n",
            "buying buy VERB VBG pcomp xxxx True False\n",
            "U.K. U.K. PROPN NNP compound X.X. False False\n",
            "startup startup NOUN NN dobj xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "$ $ SYM $ quantmod $ False False\n",
            "1 1 NUM CD compound d False False\n",
            "billion billion NUM CD pobj xxxx True False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRz-ASAbpz6D",
        "colab_type": "code",
        "outputId": "07e96af3-0c49-4583-f7dc-5f4b8ba03b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOd0tmsFxen_",
        "colab_type": "text"
      },
      "source": [
        "# Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flDNN0Hipz3i",
        "colab_type": "code",
        "outputId": "377fbc74-517f-4672-9507-957a7234904b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr2anP3dpz09",
        "colab_type": "code",
        "outputId": "669d37f1-1cee-4b8a-d6c8-73911bbfc451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokens = nlp(\"dog cat banana afskfsd\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog True 26.217972 True\n",
            "cat True 25.595104 True\n",
            "banana True 27.524435 True\n",
            "afskfsd True 23.851799 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TWSnRrJpzaf",
        "colab_type": "code",
        "outputId": "160fa7d5-9682-46e6-ab57-23c04273c260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # make sure to use larger model!\n",
        "tokens = nlp(\"dog cat banana\")\n",
        "\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog dog 1.0\n",
            "dog cat 0.70593494\n",
            "dog banana 0.47661957\n",
            "cat dog 0.70593494\n",
            "cat cat 1.0\n",
            "cat banana 0.48634458\n",
            "banana dog 0.47661957\n",
            "banana cat 0.48634458\n",
            "banana banana 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXwQ9352zYKS",
        "colab_type": "text"
      },
      "source": [
        "# Vocab, hashes and lexemes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2KchlpIpzXT",
        "colab_type": "code",
        "outputId": "c7a5e653-23e2-4dfb-c2c5-7f0f8421974e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"I\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[4690420944186131903])  # 'coffee'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4690420944186131903\n",
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJunLw9pzSp",
        "colab_type": "code",
        "outputId": "085913b8-5c7e-41c6-9685-f0f61112dfd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I 4690420944186131903 X I I True False True en\n",
            "love 3702023516439754181 xxxx l ove True False False en\n",
            "coffee 3197928453018144401 xxxx c fee True False False en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0gFKwVVpzPv",
        "colab_type": "code",
        "outputId": "d95c69d8-7e9b-457d-a141-3bf1e210c581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")  # Original Doc\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n",
        "\n",
        "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
        "# empty_doc.vocab.strings[3197928453018144401] will raise an error :(\n",
        "\n",
        "empty_doc.vocab.strings.add(\"coffee\")  # Add \"coffee\" and generate hash\n",
        "print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍\n",
        "\n",
        "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
        "print(new_doc.vocab.strings[3197928453018144401])  # 'coffee' 👍"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3197928453018144401\n",
            "coffee\n",
            "coffee\n",
            "coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZsjOdNv1b7h",
        "colab_type": "text"
      },
      "source": [
        "# Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNcSfl3B1xyF",
        "colab_type": "code",
        "outputId": "d8581e91-8a99-4d90-c58a-a540a5a41e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement KnowledgeBase (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for KnowledgeBase\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gDkE5ggpzK-",
        "colab_type": "code",
        "outputId": "975ec299-3d9f-4640-c1e3-8c700dd8f4e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "​\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
        "​\n",
        "print()\n",
        "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
        "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-dd99d45b7ae8>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyc4GpzZ5VKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38oCR8aTnuPL",
        "colab_type": "code",
        "outputId": "12597d69-f551-4a37-b8c1-cee21b31411e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "\n",
        "candidates = kb.get_candidates(\"Douglas\")\n",
        "for c in candidates:\n",
        "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Q1004791 0.6000000238418579 [0.0, 3.0, 5.0]\n",
            "  Q42 0.10000000149011612 [1.0, 9.0, -3.0]\n",
            "  Q5301561 0.20000000298023224 [-2.0, 4.0, 2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-OJWdP6AxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9o1ky7f5V_e",
        "colab_type": "code",
        "outputId": "1234f2ca-aad6-42f6-9488-db4f7c3e8d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"Hello, world. Here are two sentences.\")\n",
        "print([t.text for t in doc])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zETkjfGT6mRh",
        "colab_type": "code",
        "outputId": "446a01d3-f6ba-4a1b-957d-f5f3018c6b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "doc = nlp(\"Peach emoji is where it has always been. Peach is the superior \"\n",
        "          \"emoji. It's outranking eggplant 🍑 \")\n",
        "print(doc[0].text)          # 'Peach'\n",
        "print(doc[1].text)          # 'emoji'\n",
        "print(doc[-1].text)         # '🍑'\n",
        "print(doc[17:19].text)      # 'outranking eggplant'\n",
        "​\n",
        "print(sentences[1].text)    # 'Peach is the superior emoji.'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peach\n",
            "emoji\n",
            "🍑\n",
            "outranking eggplant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwirmkJI7_1x",
        "colab_type": "text"
      },
      "source": [
        "#**Get part-of-speech tags and flags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Is2JAhh7xPR",
        "colab_type": "code",
        "outputId": "0db62207-b5a4-49db-a203-aea1ca28b36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        " import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "apple = doc[0]\n",
        "print(\"Fine-grained POS tag\", apple.pos_, apple.pos)\n",
        "print(\"Coarse-grained POS tag\", apple.tag_, apple.tag)\n",
        "print(\"Word shape\", apple.shape_, apple.shape)\n",
        "print(\"Alphabetic characters?\", apple.is_alpha)\n",
        "print(\"Punctuation mark?\", apple.is_punct)\n",
        "\n",
        "billion = doc[10]\n",
        "print(\"Digit?\", billion.is_digit)\n",
        "print(\"Like a number?\", billion.like_num)\n",
        "print(\"Like an email address?\", billion.like_email)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine-grained POS tag PROPN 96\n",
            "Coarse-grained POS tag NNP 15794550382381185553\n",
            "Word shape Xxxxx 16072095006890171862\n",
            "Alphabetic characters? True\n",
            "Punctuation mark? False\n",
            "Digit? False\n",
            "Like a number? True\n",
            "Like an email address? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTHW426x8mpI",
        "colab_type": "text"
      },
      "source": [
        "# Recognize and update named entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnZN-tmF8QfF",
        "colab_type": "code",
        "outputId": "ff11359d-3f07-4c45-b4f2-7163cf3b7afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "doc = nlp(\"FB is hiring a new VP of global policy\")\n",
        "doc.ents = [Span(doc, 0, 1, label=\"ORG\")]\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "San Francisco 0 13 GPE\n",
            "FB 0 2 ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Q4Y21J8Qa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "train_data = [(\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for i in range(10):\n",
        "        random.shuffle(train_data)\n",
        "        for text, annotations in train_data:\n",
        "            nlp.update([text], [annotations], sgd=optimizer)\n",
        "nlp.to_disk(\"/model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXuYzeyo8QXu",
        "colab_type": "code",
        "outputId": "ecbcf9dc-d5eb-4d13-ed72-50279e9655ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc_dep = nlp(\"This is a sentence.\")\n",
        "displacy.serve(doc_dep, style=\"dep\")\n",
        "\n",
        "doc_ent = nlp(\"When Sebastian Thrun started working on self-driving cars at Google \"\n",
        "              \"in 2007, few people outside of the company took him seriously.\")\n",
        "displacy.serve(doc_ent, style=\"ent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYuqipk18QUX",
        "colab_type": "code",
        "outputId": "7211efcd-de60-4e7f-c1d1-e82f9afc4702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "import spacy\n",
        "​\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
        "​\n",
        "apple = doc[0]\n",
        "banana = doc[2]\n",
        "pasta = doc[6]\n",
        "hippo = doc[8]\n",
        "​\n",
        "print(\"apple <-> banana\", apple.similarity(banana))\n",
        "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
        "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-2f23373d9b49>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap-LDk8b8QRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}